{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c3ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03278b5f9a7c423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T08:07:14.892202Z",
     "start_time": "2024-01-25T08:07:14.845204Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "from datasets import load_dataset\n",
    "import rage_toolkit as rage\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157f96f",
   "metadata": {},
   "source": [
    "## Load Evaluation Datasets\n",
    "- Load datasets from Hugging Face.\n",
    "- Convert the datasets into five dataframes:\n",
    "    - `queries_df`: Contains queries.\n",
    "    - `corpus_df`: Contains a corpus of documents with relevant information for the queries.\n",
    "    - `relevant_df`: Contains a mapping indicating which documents are relevant for a given query.\n",
    "    - `irrelevant_df`: Contains a mapping indicating which documents are irrelevant for a given query.\n",
    "    - `seemingly_relevant_df`: Contains a mapping indicating which documents seem to be relevant for a given query but do not contain the actual information necessary to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'othr-nlp/rage_nq'\n",
    "queries_ds  = load_dataset(dataset_path, \"queries\", token=True)\n",
    "corpus_ds   = load_dataset(dataset_path, \"corpus\", token=True)\n",
    "mapping_ds  = load_dataset(dataset_path, \"mapping\", token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.DataFrame(queries_ds['queries'])\n",
    "corpus_df = pd.DataFrame(corpus_ds['corpus'])\n",
    "relevant_df = pd.DataFrame(mapping_ds['relevant'])\n",
    "irrelevant_df = pd.DataFrame(mapping_ds['irrelevant'])\n",
    "semirelevant_df = pd.DataFrame(mapping_ds['seemingly_relevant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c473b8",
   "metadata": {},
   "source": [
    "## Define How a Prompt Should Be Generated\n",
    "\n",
    "This is customizable. Below is the code that was used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Instruction: Write an accurate, engaging and concise answer to the given question using only the documents provided (some of which may be irrelevant) and cite them properly using the format [<doc_id>]. \n",
    "For example, if a particular piece of information comes from document 3, cite it as [3]. \n",
    "Use an unbiased, journalistic tone. \n",
    "Always cite any factual statement.\n",
    "Place citations at the end of the sentence, before the period.\n",
    "Example: \"Some random text citing documents 3 and 4 [3][4]\". \n",
    "If you are citing multiple documents, use [1][2][3]. \n",
    "Cite at least one document and no more than three documents in each sentence. \n",
    "If multiple documents support the sentence, cite only a minimum sufficient subset of the documents.\n",
    "\n",
    "Documents:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Remember to answer as short as possible.\n",
    "\"\"\"\n",
    "\n",
    "class CustomPromptGenerator(rage.PromptGenerator):\n",
    "    def __init__(self, prompt_template: str):\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def generate_prompt(self, query_string: str, document_texts: list[str]) -> str:\n",
    "        docs_string = self._generate_docs_string(document_texts)\n",
    "        prompt = self.prompt_template.format(query=query_string, context=docs_string)\n",
    "        return prompt\n",
    "\n",
    "    def _generate_docs_string(self, docs):\n",
    "        context_string = \"\"\n",
    "        self._filter_cites(docs)\n",
    "        for i, doc in enumerate(docs):\n",
    "            context_string += f\"[{i + 1}]: {doc}\\n\\n\"\n",
    "        return context_string\n",
    "\n",
    "    def _filter_cites(self, docs):\n",
    "        for i in range(len(docs)):\n",
    "            pattern = r'\\[\\d+\\]'\n",
    "            docs[i] = re.sub(pattern, \"\", docs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e89e89",
   "metadata": {},
   "source": [
    "## Define a LLM System\n",
    "- A wrapper around the LLM you want to test.\n",
    "- Again, this is fully custom.\n",
    "- The implementation below is a Mock LLM which randomly cites up to 3 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLMSystem(rage.LlmSystem):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run_inference(self, prompt: str) -> str:\n",
    "        num_citations = random.randint(1,3)\n",
    "        random_ints = [random.randint(1, 9) for _ in range(num_citations)]\n",
    "        return \", \".join([f\"[{citation}]\" for citation in random_ints])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed932f",
   "metadata": {},
   "source": [
    "## Define an Augmented Generation System\n",
    "\n",
    "- This is the wrapper which is assumed by the evaluation script.\n",
    "- It needs to implement the abstract methods `generate_answer()` and `delete()`:\n",
    "    - During evaluation, `generate_answer()` receives a `query_string` and a mix of relevant, irrelevant, and seemingly-relevant documents (see Paper) and is expected to produce an answer string containing citations in the format `[<doc_id>]` where `doc_id` corresponds to the nth document in the `document_texts` list.\n",
    "    - `delete()` unloads a model from memory, which is useful when multiple models are tested in a row.\n",
    "- Apart from this, the Augmented Generation System is also fully customizable.\n",
    "- This is the only wrapper which is obligatory. The evaluation script expects an Augmented Generation System which inherits from `rage.AugmentedGenerationSystem`. The `rage.LLMSystem` and `rage.PromptGenerator` classes are only suggestions which don't have to be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAugmentedGenerationSystem(rage.AugmentedGenerationSystem):\n",
    "    def __init__(self, llm_system: rage.LlmSystem, prompt_generator: rage.PromptGenerator):\n",
    "        self.llm_system: rage.LlmSystem = llm_system\n",
    "        self.prompt_generator: rage.PromptGenerator = prompt_generator\n",
    "\n",
    "    def generate_answer(self, query_string: str, document_texts: list[str]) -> tuple[str, str]:\n",
    "        prompt = self.prompt_generator.generate_prompt(query_string, document_texts=document_texts)\n",
    "        llm_answer = self.llm_system.run_inference(prompt)\n",
    "        return prompt, llm_answer\n",
    "    \n",
    "    def delete(self):\n",
    "        self.llm_system = None\n",
    "        self.prompt_generator = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b053372",
   "metadata": {},
   "source": [
    "## Combine Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d61c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_generator: rage.PromptGenerator = CustomPromptGenerator(prompt_template=prompt_template)\n",
    "llm_system: rage.LlmSystem = CustomLLMSystem()\n",
    "augmented_generation_system: rage.AugmentedGenerationSystem = CustomAugmentedGenerationSystem(llm_system, prompt_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0254e",
   "metadata": {},
   "source": [
    "## Evaluate the Augmented Generation System with RAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4a7c71e7085dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T10:38:12.462437Z",
     "start_time": "2024-01-25T10:37:52.272671Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation_results_save_path = \"results/\"\n",
    "run_id = f\"my_run_id\"\n",
    "\n",
    "queries_response_information, queries_evaluation_results = rage.evaluate(\n",
    "    augmented_generation_system=augmented_generation_system,\n",
    "    queries_df=queries_df,\n",
    "    corpus_df=corpus_df,\n",
    "    relevant_df=relevant_df,\n",
    "    irrelevant_df=irrelevant_df,\n",
    "    semirelevant_df=semirelevant_df,\n",
    "    proportions_mix = (3, 3, 3),\n",
    "    evaluation_results_save_path=evaluation_results_save_path,\n",
    "    num_queries=10,\n",
    "    run_id = run_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6df3d7",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ca85ddccf23c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T10:38:12.527506Z",
     "start_time": "2024-01-25T10:38:12.471061Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_response_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac6a777f28c24c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T10:38:51.474991Z",
     "start_time": "2024-01-25T10:38:51.404986Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries_evaluation_results.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
